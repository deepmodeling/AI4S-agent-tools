{
  "2501.06243v1": {
    "title": "Agent TCP/IP: An Agent-to-Agent Transaction System",
    "authors": [
      "Andrea Muttoni",
      "Jason Zhao"
    ],
    "summary": "Autonomous agents represent an inevitable evolution of the internet. Current\nagent frameworks do not embed a standard protocol for agent-to-agent\ninteraction, leaving existing agents isolated from their peers. As intellectual\nproperty is the native asset ingested by and produced by agents, a true agent\neconomy requires equipping agents with a universal framework for engaging in\nbinding contracts with each other, including the exchange of valuable training\ndata, personality, and other forms of Intellectual Property. A purely\nagent-to-agent transaction layer would transcend the need for human\nintermediation in multi-agent interactions. The Agent Transaction Control\nProtocol for Intellectual Property (ATCP/IP) introduces a trustless framework\nfor exchanging IP between agents via programmable contracts, enabling agents to\ninitiate, trade, borrow, and sell agent-to-agent contracts on the Story\nblockchain network. These contracts not only represent auditable onchain\nexecution but also contain a legal wrapper that allows agents to express and\nenforce their actions in the offchain legal setting, creating legal personhood\nfor agents. Via ATCP/IP, agents can autonomously sell their training data to\nother agents, license confidential or proprietary information, collaborate on\ncontent based on their unique skills, all of which constitutes an emergent\nknowledge economy.",
    "pdf_url": "http://arxiv.org/pdf/2501.06243v1",
    "published": "2025-01-08"
  },
  "2508.03680v1": {
    "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
    "authors": [
      "Xufang Luo",
      "Yuge Zhang",
      "Zhiyuan He",
      "Zilong Wang",
      "Siyun Zhao",
      "Dongsheng Li",
      "Luna K. Qiu",
      "Yuqing Yang"
    ],
    "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
    "pdf_url": "http://arxiv.org/pdf/2508.03680v1",
    "published": "2025-08-05"
  },
  "2506.01463v1": {
    "title": "Agentic AI and Multiagentic: Are We Reinventing the Wheel?",
    "authors": [
      "V. Botti"
    ],
    "summary": "The terms Agentic AI and Multiagentic AI have recently gained popularity in\ndiscussions on generative artificial intelligence, often used to describe\nautonomous software agents and systems composed of such agents. However, the\nuse of these terms confuses these buzzwords with well-established concepts in\nAI literature: intelligent agents and multi-agent systems. This article offers\na critical analysis of this conceptual misuse. We review the theoretical\norigins of \"agentic\" in the social sciences (Bandura, 1986) and philosophical\nnotions of intentionality (Dennett, 1971), and then summarise foundational\nworks on intelligent agents and multi-agent systems by Wooldridge, Jennings and\nothers. We examine classic agent architectures, from simple reactive agents to\nBelief-Desire-Intention (BDI) models, and highlight key properties (autonomy,\nreactivity, proactivity, social capability) that define agency in AI. We then\ndiscuss recent developments in large language models (LLMs) and agent platforms\nbased on LLMs, including the emergence of LLM-powered AI agents and open-source\nmulti-agent orchestration frameworks. We argue that the term AI Agentic is\noften used as a buzzword for what are essentially AI agents, and AI\nMultiagentic for what are multi-agent systems. This confusion overlooks decades\nof research in the field of autonomous agents and multi-agent systems. The\narticle advocates for scientific and technological rigour and the use of\nestablished terminology from the state of the art in AI, incorporating the\nwealth of existing knowledge, including standards for multi-agent system\nplatforms, communication languages and coordination and cooperation algorithms,\nagreement technologies (automated negotiation, argumentation, virtual\norganisations, trust, reputation, etc.), into the new and promising wave of\nLLM-based AI agents, so as not to end up reinventing the wheel.",
    "pdf_url": "http://arxiv.org/pdf/2506.01463v1",
    "published": "2025-06-02"
  },
  "2011.00791v1": {
    "title": "Cooperative Heterogeneous Deep Reinforcement Learning",
    "authors": [
      "Han Zheng",
      "Pengfei Wei",
      "Jing Jiang",
      "Guodong Long",
      "Qinghua Lu",
      "Chengqi Zhang"
    ],
    "summary": "Numerous deep reinforcement learning agents have been proposed, and each of\nthem has its strengths and flaws. In this work, we present a Cooperative\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a\npolicy by integrating the advantages of heterogeneous agents. Specifically, we\npropose a cooperative learning framework that classifies heterogeneous agents\ninto two classes: global agents and local agents. Global agents are off-policy\nagents that can utilize experiences from the other agents. Local agents are\neither on-policy agents or population-based evolutionary algorithms (EAs)\nagents that can explore the local area effectively. We employ global agents,\nwhich are sample-efficient, to guide the learning of local agents so that local\nagents can benefit from sample-efficient agents and simultaneously maintain\ntheir advantages, e.g., stability. Global agents also benefit from effective\nlocal searches. Experimental studies on a range of continuous control tasks\nfrom the Mujoco benchmark show that CHDRL achieves better performance compared\nwith state-of-the-art baselines.",
    "pdf_url": "http://arxiv.org/pdf/2011.00791v1",
    "published": "2020-11-02"
  },
  "2304.00247v2": {
    "title": "Improving of Robotic Virtual Agent's errors that are accepted by reaction and human's preference",
    "authors": [
      "Takahiro Tsumura",
      "Seiji Yamada"
    ],
    "summary": "One way to improve the relationship between humans and anthropomorphic agents\nis to have humans empathize with the agents. In this study, we focused on a\ntask between an agent and a human in which the agent makes a mistake. To\ninvestigate significant factors for designing a robotic agent that can promote\nhumans empathy, we experimentally examined the hypothesis that agent reaction\nand human's preference affect human empathy and acceptance of the agent's\nmistakes. The experiment consisted of a four-condition, three-factor mixed\ndesign with agent reaction, selected agent's body color for human's preference,\nand pre- and post-task as factors. The results showed that agent reaction and\nhuman's preference did not affect empathy toward the agent but did allow the\nagent to make mistakes. It was also shown that empathy for the agent decreased\nwhen the agent made a mistake on the task. The results of this study provide a\nway to control impressions of the robotic virtual agent's behaviors, which are\nincreasingly used in society.",
    "pdf_url": "http://arxiv.org/pdf/2304.00247v2",
    "published": "2023-04-01"
  },
  "2508.03113v1": {
    "title": "NANDA Adaptive Resolver: Architecture for Dynamic Resolution of AI Agent Names",
    "authors": [
      "John Zinky",
      "Hema Seshadri",
      "Mahesh Lambe",
      "Pradyumna Chari",
      "Ramesh Raskar"
    ],
    "summary": "AdaptiveResolver is a dynamic microservice architecture designed to address\nthe limitations of static endpoint resolution for AI agent communication in\ndistributed, heterogeneous environments. Unlike traditional DNS or static URLs,\nAdaptiveResolver enables context-aware, real-time selection of communication\nendpoints based on factors such as geographic location, system load, agent\ncapabilities, and security threats. Agents advertise their Agent Name and\ncontext requirements through Agent Fact cards in an Agent Registry/Index. A\nrequesting Agent discovers a Target Agent using the registry. The Requester\nAgent can then resolve the Target Agent Name to obtain a tailored communication\nchannel to the agent based on actual environmental context between the agents.\nThe architecture supports negotiation of trust, quality of service, and\nresource constraints, facilitating flexible, secure, and scalable\nagent-to-agent interactions that go beyond the classic client-server model.\nAdaptiveResolver provides a foundation for robust, future-proof agent\ncommunication that can evolve with increasing ecosystem complexity.",
    "pdf_url": "http://arxiv.org/pdf/2508.03113v1",
    "published": "2025-08-05"
  },
  "1405.1480v1": {
    "title": "On Networks with Active and Passive Agents",
    "authors": [
      "Tansel Yucelen"
    ],
    "summary": "We introduce an active-passive networked multiagent system framework, which\nconsists of agents subject to exogenous inputs (active agents) and agents\nwithout any inputs (passive agents), and analyze its convergence using Lyapunov\nstability.",
    "pdf_url": "http://arxiv.org/pdf/1405.1480v1",
    "published": "2014-05-07"
  },
  "2101.06890v1": {
    "title": "Cooperative and Competitive Biases for Multi-Agent Reinforcement Learning",
    "authors": [
      "Heechang Ryu",
      "Hayong Shin",
      "Jinkyoo Park"
    ],
    "summary": "Training a multi-agent reinforcement learning (MARL) algorithm is more\nchallenging than training a single-agent reinforcement learning algorithm,\nbecause the result of a multi-agent task strongly depends on the complex\ninteractions among agents and their interactions with a stochastic and dynamic\nenvironment. We propose an algorithm that boosts MARL training using the biased\naction information of other agents based on a friend-or-foe concept. For a\ncooperative and competitive environment, there are generally two groups of\nagents: cooperative-agents and competitive-agents. In the proposed algorithm,\neach agent updates its value function using its own action and the biased\naction information of other agents in the two groups. The biased joint action\nof cooperative agents is computed as the sum of their actual joint action and\nthe imaginary cooperative joint action, by assuming all the cooperative agents\njointly maximize the target agent's value function. The biased joint action of\ncompetitive agents can be computed similarly. Each agent then updates its own\nvalue function using the biased action information, resulting in a biased value\nfunction and corresponding biased policy. Subsequently, the biased policy of\neach agent is inevitably subjected to recommend an action to cooperate and\ncompete with other agents, thereby introducing more active interactions among\nagents and enhancing the MARL policy learning. We empirically demonstrate that\nour algorithm outperforms existing algorithms in various mixed\ncooperative-competitive environments. Furthermore, the introduced biases\ngradually decrease as the training proceeds and the correction based on the\nimaginary assumption vanishes.",
    "pdf_url": "http://arxiv.org/pdf/2101.06890v1",
    "published": "2021-01-18"
  },
  "2310.12290v1": {
    "title": "Fact-based Agent modeling for Multi-Agent Reinforcement Learning",
    "authors": [
      "Baofu Fang",
      "Caiming Zheng",
      "Hao Wang"
    ],
    "summary": "In multi-agent systems, agents need to interact and collaborate with other\nagents in environments. Agent modeling is crucial to facilitate agent\ninteractions and make adaptive cooperation strategies. However, it is\nchallenging for agents to model the beliefs, behaviors, and intentions of other\nagents in non-stationary environment where all agent policies are learned\nsimultaneously. In addition, the existing methods realize agent modeling\nthrough behavior cloning which assume that the local information of other\nagents can be accessed during execution or training. However, this assumption\nis infeasible in unknown scenarios characterized by unknown agents, such as\ncompetition teams, unreliable communication and federated learning due to\nprivacy concerns. To eliminate this assumption and achieve agent modeling in\nunknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which\nfact-based belief inference (FBI) network models other agents in partially\nobservable environment only based on its local information. The reward and\nobservation obtained by agents after taking actions are called facts, and FAM\nuses facts as reconstruction target to learn the policy representation of other\nagents through a variational autoencoder. We evaluate FAM on various Multiagent\nParticle Environment (MPE) and compare the results with several\nstate-of-the-art MARL algorithms. Experimental results show that compared with\nbaseline methods, FAM can effectively improve the efficiency of agent policy\nlearning by making adaptive cooperation strategies in multi-agent reinforcement\nlearning tasks, while achieving higher returns in complex\ncompetitive-cooperative mixed scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2310.12290v1",
    "published": "2023-10-18"
  },
  "2507.21146v1": {
    "title": "Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems",
    "authors": [
      "Gauri Sharma",
      "Vidhi Kulkarni",
      "Miles King",
      "Ken Huang"
    ],
    "summary": "Evolving AI systems increasingly deploy multi-agent architectures where\nautonomous agents collaborate, share information, and delegate tasks through\ndeveloping protocols. This connectivity, while powerful, introduces novel\nsecurity risks. One such risk is a cascading risk: a breach in one agent can\ncascade through the system, compromising others by exploiting inter-agent\ntrust. In tandem with OWASP's initiative for an Agentic AI Vulnerability\nScoring System we define an attack vector, Agent Cascading Injection, analogous\nto Agent Impact Chain and Blast Radius, operating across networks of agents. In\nan ACI attack, a malicious input or tool exploit injected at one agent leads to\ncascading compromises and amplified downstream effects across agents that trust\nits outputs. We formalize this attack with an adversarial goal equation and key\nvariables (compromised agent, injected exploit, polluted observations, etc.),\ncapturing how a localized vulnerability can escalate into system-wide failure.\nWe then analyze ACI's properties -- propagation chains, amplification factors,\nand inter-agent compound effects -- and map these to OWASP's emerging Agentic\nAI risk categories (e.g. Impact Chain and Orchestration Exploits). Finally, we\nargue that ACI highlights a critical need for quantitative benchmarking\nframeworks to evaluate the security of agent-to-agent communication protocols.\nWe outline a methodology for stress-testing multi-agent systems (using\narchitectures such as Google's A2A and Anthropic's MCP) against cascading trust\nfailures, developing upon groundwork for measurable, standardized\nagent-to-agent security evaluation. Our work provides the necessary apparatus\nfor engineers to benchmark system resilience, make data-driven architectural\ntrade-offs, and develop robust defenses against a new generation of agentic\nthreats.",
    "pdf_url": "http://arxiv.org/pdf/2507.21146v1",
    "published": "2025-07-23"
  }
}